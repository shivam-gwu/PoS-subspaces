{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d79fc7",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "**[1. PoS subspaces](#PoS-subspaces)**\n",
    "  * [1.1. Subspace visualisation](##PoS-subspace-visualisation)\n",
    "  * [1.2. Noun/Adjective subspace projection](##Adj/Noun-subspace-projection)\n",
    "  * [1.3. Style-blocking adjective projections](##Style-blocking-adjective-projection)\n",
    "\n",
    "**[2. Custom visual theme blocking](#Custom-subspace-projection)**\n",
    "\n",
    "----\n",
    "\n",
    "Code builds off Paella's notebook from: https://github.com/dome272/Paella/blob/1baf86966f847661378b84c9b27386c12ab51a1c/paella_inference.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a69f2",
   "metadata": {},
   "source": [
    "# TTIM model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip uninstall torch --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5dd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fa465",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.0+cu117 torchvision==0.14.0+cu117 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23de4adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    3475 MB |    3476 MB |    3546 MB |   73088 KB |\\n|       from large pool |    3469 MB |    3469 MB |    3536 MB |   69120 KB |\\n|       from small pool |       5 MB |       6 MB |       9 MB |    3968 KB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    3475 MB |    3476 MB |    3546 MB |   73088 KB |\\n|       from large pool |    3469 MB |    3469 MB |    3536 MB |   69120 KB |\\n|       from small pool |       5 MB |       6 MB |       9 MB |    3968 KB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    3540 MB |    3540 MB |    3540 MB |       0 B  |\\n|       from large pool |    3532 MB |    3532 MB |    3532 MB |       0 B  |\\n|       from small pool |       8 MB |       8 MB |       8 MB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   64356 KB |   81753 KB |  571683 KB |  507327 KB |\\n|       from large pool |   64132 KB |   80516 KB |  560384 KB |  496252 KB |\\n|       from small pool |     224 KB |    2763 KB |   11299 KB |   11075 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     751    |     760    |     882    |     131    |\\n|       from large pool |     237    |     237    |     265    |      28    |\\n|       from small pool |     514    |     523    |     617    |     103    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     751    |     760    |     882    |     131    |\\n|       from large pool |     237    |     237    |     265    |      28    |\\n|       from small pool |     514    |     523    |     617    |     103    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     173    |     173    |     173    |       0    |\\n|       from large pool |     169    |     169    |     169    |       0    |\\n|       from small pool |       4    |       4    |       4    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      48    |      51    |      76    |      28    |\\n|       from large pool |      46    |      47    |      65    |      19    |\\n|       from small pool |       2    |       5    |      11    |       9    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e56b3494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b7b4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: you need to run the commands below firstly to download the TTIM checkpoints\n",
    "\n",
    "# !wget https://huggingface.co/dome272/Paella/resolve/main/paella_v3.pt\n",
    "# !wget https://huggingface.co/dome272/Paella/resolve/main/prior_v1.pt\n",
    "# !wget https://huggingface.co/dome272/Paella/resolve/main/vqgan_f4.pt\n",
    "# !mkdir Paella/models\n",
    "# !mv -t Paella/models paella_v3.pt prior_v1.pt vqgan_f4.pt\n",
    "\n",
    "!pip install git+https://github.com/pabloppp/pytorch-tools\n",
    "!pip install git+https://github.com/shivam-gwu/Arroz-Con-Cosas\n",
    "!pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4babc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984e3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0fdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['HF_HOME'] = '/onyx/data/p143/james/'\n",
    "\n",
    "# specify the path to the Paella repo\n",
    "ppath = '/onyx/data/p143/james/PoS-subspaces/experiments/Paella'\n",
    "sys.path.append(ppath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05c89a6-4648-46e2-90c7-cf7c1b0dd861",
   "metadata": {
    "id": "788a2a72",
    "outputId": "702cbb5b-8c71-4b33-e86e-f84b718b7a1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import open_clip\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from src.vqgan import VQModel\n",
    "from open_clip import tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.modules import Paella\n",
    "from arroz import Diffuzz, PriorModel\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "from utils.alter_attention import replace_attention_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85ddc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2e1f26-b4ca-4e11-947d-be80196d440f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paella scaffold: init\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import open_clip\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from src.vqgan import VQModel\n",
    "from open_clip import tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.modules import Paella\n",
    "from arroz import Diffuzz, PriorModel\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "from utils.alter_attention import replace_attention_layers\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "def sample(model, seed, init_noise, model_inputs, latent_shape, unconditional_inputs=None, init_x=None, steps=12, renoise_steps=None, temperature = (0.7, 0.3), cfg=(8.0, 8.0), mode = 'multinomial', t_start=1.0, t_end=0.0, sampling_conditional_steps=None, sampling_quant_steps=None, attn_weights=None): # 'quant', 'multinomial', 'argmax'\n",
    "    device = unconditional_inputs[\"byt5\"].device\n",
    "    if sampling_conditional_steps is None:\n",
    "        sampling_conditional_steps = steps\n",
    "    if sampling_quant_steps is None:\n",
    "        sampling_quant_steps = steps\n",
    "    if renoise_steps is None:\n",
    "        renoise_steps = steps-1\n",
    "    if unconditional_inputs is None:\n",
    "        unconditional_inputs = {k: torch.zeros_like(v) for k, v in model_inputs.items()}\n",
    "    intermediate_images = []\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        random.seed(seed) ; torch.manual_seed(seed) ; np.random.seed(seed); torch.cuda.manual_seed(seed) ;\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        if init_x != None:\n",
    "            sampled = init_x\n",
    "        else:\n",
    "            sampled = init_noise.clone()\n",
    "        t_list = torch.linspace(t_start, t_end, steps+1)\n",
    "        temperatures = torch.linspace(temperature[0], temperature[1], steps)\n",
    "        cfgs = torch.linspace(cfg[0], cfg[1], steps)\n",
    "        for i, tv in enumerate(t_list[:steps]):\n",
    "            if i >= sampling_quant_steps:\n",
    "                mode = \"quant\"\n",
    "            t = torch.ones(latent_shape[0], device=device) * tv\n",
    "\n",
    "            logits = model(sampled, t, **model_inputs, attn_weights=attn_weights)\n",
    "            if cfg is not None and i < sampling_conditional_steps:\n",
    "                logits = logits * cfgs[i] + model(sampled, t, **unconditional_inputs) * (1-cfgs[i])\n",
    "            scores = logits.div(temperatures[i]).softmax(dim=1)\n",
    "\n",
    "            if mode == 'argmax':\n",
    "                sampled = logits.argmax(dim=1)\n",
    "            elif mode == 'multinomial':\n",
    "                sampled = scores.permute(0, 2, 3, 1).reshape(-1, logits.size(1))\n",
    "                sampled = torch.multinomial(sampled, 1)[:, 0].view(logits.size(0), *logits.shape[2:])\n",
    "            elif mode == 'quant':\n",
    "                sampled = scores.permute(0, 2, 3, 1) @ vqmodel.vquantizer.codebook.weight.data\n",
    "                sampled = vqmodel.vquantizer.forward(sampled, dim=-1)[-1]\n",
    "            else:\n",
    "                raise Exception(f\"Mode '{mode}' not supported, use: 'quant', 'multinomial' or 'argmax'\")\n",
    "\n",
    "            intermediate_images.append(sampled)\n",
    "\n",
    "            if i < renoise_steps:\n",
    "                t_next = torch.ones(latent_shape[0], device=device) * t_list[i+1]\n",
    "                sampled = model.add_noise(sampled, t_next, random_x=init_noise.clone())[0]\n",
    "                intermediate_images.append(sampled)\n",
    "    return sampled, intermediate_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec61f2",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.pi = math.pi\n",
    "\n",
    "def saveimages(imgs, name, base=\"orth-outputs\", **kwargs):\n",
    "    name = name.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "    path = os.path.join(base, name + \".jpg\")\n",
    "    torchvision.utils.save_image(imgs, path, **kwargs)\n",
    "\n",
    "def showimages(imgs, rows=False, title=None, fontsize=20, **kwargs):\n",
    "    plt.figure(figsize=(kwargs.get(\"width\", 32), kwargs.get(\"height\", 32)))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if imgs.dtype == torch.float32 or imgs.dtype == torch.float64:\n",
    "        min_val = torch.min(imgs)\n",
    "        max_val = torch.max(imgs)\n",
    "        imgs = (imgs - min_val) / (max_val - min_val)\n",
    "\n",
    "    if title: plt.title(title, fontsize=fontsize)\n",
    "\n",
    "    # if rows is True, then the images are arranged in rows\n",
    "    if rows:\n",
    "        plt.imshow(torch.cat([torch.cat([i for i in row], dim=-1) for row in imgs], dim=-2).permute(1, 2, 0).cpu())\n",
    "    else:\n",
    "        plt.imshow(torch.cat([torch.cat([i for i in imgs], dim=-1)], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d0b35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae00f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12296\\2224583178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba4c0508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    3476 MB |    3476 MB |    3546 MB |   71914 KB |\\n|       from large pool |    3469 MB |    3469 MB |    3536 MB |   69120 KB |\\n|       from small pool |       6 MB |       6 MB |       9 MB |    2794 KB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    3476 MB |    3476 MB |    3546 MB |   71914 KB |\\n|       from large pool |    3469 MB |    3469 MB |    3536 MB |   69120 KB |\\n|       from small pool |       6 MB |       6 MB |       9 MB |    2794 KB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    3540 MB |    3540 MB |    3540 MB |       0 B  |\\n|       from large pool |    3532 MB |    3532 MB |    3532 MB |       0 B  |\\n|       from small pool |       8 MB |       8 MB |       8 MB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   65229 KB |   81753 KB |  571085 KB |  505855 KB |\\n|       from large pool |   64132 KB |   80516 KB |  560384 KB |  496252 KB |\\n|       from small pool |    1097 KB |    2763 KB |   10701 KB |    9603 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     760    |     760    |     882    |     122    |\\n|       from large pool |     237    |     237    |     265    |      28    |\\n|       from small pool |     523    |     523    |     617    |      94    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     760    |     760    |     882    |     122    |\\n|       from large pool |     237    |     237    |     265    |      28    |\\n|       from small pool |     523    |     523    |     617    |      94    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     173    |     173    |     173    |       0    |\\n|       from large pool |     169    |     169    |     169    |       0    |\\n|       from small pool |       4    |       4    |       4    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      48    |      50    |      73    |      25    |\\n|       from large pool |      46    |      47    |      65    |      19    |\\n|       from small pool |       2    |       5    |       8    |       6    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0153cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_per_process_memory_fraction(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c7c626",
   "metadata": {
    "id": "34c7c626",
    "outputId": "16108313-6b3d-4751-d84b-a92d7f8ebf68",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.39 GiB already allocated; 0 bytes free; 4.00 GiB allowed; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28200\\2038242419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mt5_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mvqmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVQModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mvqmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"vqgan_f4.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mvqmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 987\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    983\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    984\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.39 GiB already allocated; 0 bytes free; 4.00 GiB allowed; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Paella scaffold: init\n",
    "with torch.no_grad():\n",
    "    model_path = \"models\"\n",
    "\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(256),\n",
    "        torchvision.transforms.CenterCrop(256),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    def encode(x):\n",
    "        return vqmodel.encode(x, quantize=True)[2]\n",
    "\n",
    "    def decode(img_seq):\n",
    "        return vqmodel.decode_indices(img_seq)\n",
    "\n",
    "    def embed_t5(text, t5_tokenizer, t5_model, device=\"cuda\"):\n",
    "        t5_tokens = t5_tokenizer(text, padding=\"longest\", return_tensors=\"pt\", max_length=768, truncation=True).input_ids.to(device)\n",
    "        t5_embeddings = t5_model(input_ids=t5_tokens).last_hidden_state\n",
    "        return t5_embeddings\n",
    "\n",
    "    vqmodel = VQModel().to(device)\n",
    "    vqmodel.load_state_dict(torch.load(os.path.join(model_path, \"vqgan_f4.pt\"), map_location=device))\n",
    "    vqmodel.eval().requires_grad_(False)\n",
    "\n",
    "    clip_model, _, _ = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
    "    clip_model = clip_model.to(device).eval().requires_grad_(False)\n",
    "\n",
    "    clip_preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "        torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-xl\")  # change with \"t5-b3\" for the 10GB model LoL\n",
    "    t5_model = T5EncoderModel.from_pretrained(\"google/byt5-xl\").to(device).requires_grad_(False)\n",
    "\n",
    "    prior_ckpt = torch.load(os.path.join(model_path, \"prior_v1.pt\"), map_location=device)\n",
    "    prior = PriorModel().to(device)\n",
    "    prior.load_state_dict(prior_ckpt)\n",
    "    prior.eval().requires_grad_(False)\n",
    "    diffuzz = Diffuzz(device=device)\n",
    "    del prior_ckpt\n",
    "\n",
    "    state_dict = torch.load(os.path.join(model_path, \"paella_v3.pt\"), map_location=device)\n",
    "    model = Paella(byt5_embd=2560).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval().requires_grad_()\n",
    "    replace_attention_layers(model)\n",
    "    model.to(device)\n",
    "    del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2503ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_28200\\1123902057.py\", line 45, in <module>\n",
      "    clip_model = load_clip_model()\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_28200\\1123902057.py\", line 21, in load_clip_model\n",
      "    clip_model, _, _ = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\factory.py\", line 399, in create_model_and_transforms\n",
      "    **model_kwargs,\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\factory.py\", line 252, in create_model\n",
      "    model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\model.py\", line 239, in __init__\n",
      "    text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\model.py\", line 215, in _build_text_tower\n",
      "    norm_layer=norm_layer,\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\transformer.py\", line 626, in __init__\n",
      "    self.init_parameters()\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\transformer.py\", line 641, in init_parameters\n",
      "    nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\init.py\", line 155, in normal_\n",
      "    return _no_grad_normal_(tensor, mean, std)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\init.py\", line 19, in _no_grad_normal_\n",
      "    return tensor.normal_(mean, std)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2099, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python37\\lib\\inspect.py\", line 1464, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"c:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python37\\lib\\linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python37\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python37\\lib\\tokenize.py\", line 447, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28200\\1123902057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mvqmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vq_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mclip_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clip_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mt5_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt5_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_t5_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28200\\1123902057.py\u001b[0m in \u001b[0;36mload_clip_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_clip_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mclip_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_clip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model_and_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ViT-H-14'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'laion2b_s32b_b79k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\factory.py\u001b[0m in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, **model_kwargs)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\factory.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, **model_kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmodel_cfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcast_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embed_dim, vision_cfg, text_cfg, quick_gelu, init_logit_scale, init_logit_bias, cast_dtype, output_dict)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_text_tower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_cfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquick_gelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcast_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\model.py\u001b[0m in \u001b[0;36m_build_text_tower\u001b[1;34m(embed_dim, text_cfg, quick_gelu, cast_dtype)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mact_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mnorm_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, context_length, vocab_size, width, heads, layers, mlp_ratio, ls_init_value, output_dim, embed_cls, no_causal_mask, pad_id, pool_type, proj_bias, act_layer, norm_layer, output_tokens)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\transformer.py\u001b[0m in \u001b[0;36minit_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfc_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproj_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2098\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2099\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2100\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2100\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2102\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1368\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1268\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m             )\n\u001b[0;32m   1270\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1125\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.transforms\n",
    "from transformers import AutoTokenizer, T5EncoderModel\n",
    "# from models import VQModel, PriorModel, Paella, replace_attention_layers\n",
    "# from openai import open_clip\n",
    "# from diffuzz import Diffuzz\n",
    "\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Define functions for model loading and data transformation\n",
    "def load_vq_model():\n",
    "    model_path = \"models\"\n",
    "    vqmodel = VQModel().to(device)\n",
    "    vqmodel.load_state_dict(torch.load(os.path.join(model_path, \"vqgan_f4.pt\"), map_location=device))\n",
    "    vqmodel.eval().requires_grad_(False)\n",
    "    return vqmodel\n",
    "\n",
    "def load_clip_model():\n",
    "    clip_model, _, _ = open_clip.create_model_and_transforms('ViT-H-14', pretrained='laion2b_s32b_b79k')\n",
    "    return clip_model.to(device).eval().requires_grad_(False)\n",
    "\n",
    "def load_t5_model():\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-xl\")\n",
    "    t5_model = T5EncoderModel.from_pretrained(\"google/byt5-xl\").to(device).requires_grad_(False)\n",
    "    return t5_tokenizer, t5_model\n",
    "\n",
    "def load_prior_model():\n",
    "    prior = PriorModel().to(device)\n",
    "    prior.load_state_dict(torch.load(os.path.join(\"models\", \"prior_v1.pt\"), map_location=device))\n",
    "    prior.eval().requires_grad_(False)\n",
    "    return prior\n",
    "\n",
    "def load_paella_model():\n",
    "    state_dict = torch.load(os.path.join(\"models\", \"paella_v3.pt\"), map_location=device)\n",
    "    model = Paella(byt5_embd=2560).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval().requires_grad_()\n",
    "    replace_attention_layers(model)\n",
    "    return model\n",
    "\n",
    "# Load models and tokenizer\n",
    "vqmodel = load_vq_model()\n",
    "clip_model = load_clip_model()\n",
    "t5_tokenizer, t5_model = load_t5_model()\n",
    "prior = load_prior_model()\n",
    "model = load_paella_model()\n",
    "\n",
    "# Define data transformations\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(256),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "clip_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "    torchvision.transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# Initialize Diffuzz\n",
    "diffuzz = Diffuzz(device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8954a29",
   "metadata": {},
   "source": [
    "# PoS subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67128114",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1eab363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nouns: 100%|| 82115/82115 [00:00<00:00, 1713756.08it/s]\n",
      "adjs: 100%|| 18156/18156 [00:00<00:00, 1628218.59it/s]\n",
      "verb: 100%|| 13767/13767 [00:00<00:00, 1534534.09it/s]\n",
      "adverbs: 100%|| 3621/3621 [00:00<00:00, 1712047.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the wordnet data\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm\n",
    "nouns_o = []\n",
    "adjectives_o = []\n",
    "verbs_o = []\n",
    "adverbs_o = []\n",
    "\n",
    "for synset in tqdm(list(wn.all_synsets(wn.NOUN)), desc='nouns'):\n",
    "    nouns_o += synset.lemma_names()\n",
    "    \n",
    "for synset in tqdm(list(wn.all_synsets(wn.ADJ)), desc='adjs'):\n",
    "    adjectives_o += synset.lemma_names()\n",
    "    \n",
    "for synset in tqdm(list(wn.all_synsets(wn.VERB)), desc='verb'):\n",
    "    verbs_o += synset.lemma_names()\n",
    "\n",
    "for synset in tqdm(list(wn.all_synsets(wn.ADV)), desc='adverbs'):\n",
    "    adverbs_o += synset.lemma_names()\n",
    "    \n",
    "nouns = [x.replace('_', ' ') for x in list(set(nouns_o).difference(set(adjectives_o).union(set(verbs_o), set(adverbs_o))))]\n",
    "adjectives = [x.replace('_', ' ') for x in list(set(adjectives_o).difference(set(nouns_o).union(set(verbs_o), set(adverbs_o))))]\n",
    "verbs = [x.replace('_', ' ') for x in list(set(verbs_o).difference(set(nouns_o).union(set(adjectives_o), set(adverbs_o))))]\n",
    "adverbs = [x.replace('_', ' ') for x in list(set(adverbs_o).difference(set(nouns_o).union(set(adjectives_o), set(verbs_o))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d9a580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: normalising inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding nouns:   0%|                                                                       | 0/112219 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12296\\1201777437.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtext_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\open_clip\\model.py\u001b[0m in \u001b[0;36mencode_text\u001b[1;34m(self, text, normalize)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mcast_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cast_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_dtype\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, n_ctx, d_model]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    160\u001b[0m         return F.embedding(\n\u001b[0;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shiva\\myenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2208\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# embed the wordnet data with CLIP\n",
    "N = []\n",
    "maxn = 500_000\n",
    "norm = 1\n",
    "\n",
    "if norm: print('WARN: normalising inputs')\n",
    "    \n",
    "for word in tqdm(nouns[:maxn], desc='encoding nouns'):\n",
    "    tokenized_text = tokenizer.tokenize([word]).to(device)\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            text_embeddings = clip_model.encode_text(tokenized_text)\n",
    "        if norm: text_embeddings /= torch.norm(text_embeddings, 2)\n",
    "        N += [text_embeddings[0]]\n",
    "N = torch.stack(N, 0).float()\n",
    "        \n",
    "A = []\n",
    "for word in tqdm(adjectives[:maxn], desc='encoding adj'):\n",
    "    tokenized_text = tokenizer.tokenize([word]).to(device)\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            text_embeddings = clip_model.encode_text(tokenized_text)\n",
    "        if norm: text_embeddings /= torch.norm(text_embeddings, 2)\n",
    "        A += [text_embeddings[0]]\n",
    "A = torch.stack(A, 0).float()\n",
    "\n",
    "V = []\n",
    "for word in tqdm(verbs[:maxn], desc='encoding verbs'):\n",
    "    tokenized_text = tokenizer.tokenize([word]).to(device)\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            text_embeddings = clip_model.encode_text(tokenized_text)\n",
    "        if norm: text_embeddings /= torch.norm(text_embeddings, 2)\n",
    "        V += [text_embeddings[0]]\n",
    "V = torch.stack(V, 0).float()\n",
    "\n",
    "AV = []\n",
    "for word in tqdm(adverbs[:maxn], desc='encoding adverbs'):\n",
    "    tokenized_text = tokenizer.tokenize([word]).to(device)\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            text_embeddings = clip_model.encode_text(tokenized_text)\n",
    "        if norm: text_embeddings /= torch.norm(text_embeddings, 2)\n",
    "        AV += [text_embeddings[0]]\n",
    "AV = torch.stack(AV, 0).float()\n",
    "\n",
    "print(N.shape, A.shape, V.shape, AV.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "# calculate the intrinsic mean\n",
    "from sphere import calculate_intrinstic_mean, logarithmic_map, exponential_map\n",
    "\n",
    "i = 1\n",
    "with torch.no_grad():\n",
    "    # map to tangent space;\n",
    "    X = torch.cat([N, A, V, AV], 0)\n",
    "    i_mean = calculate_intrinstic_mean(X, iters=1, lr=1.00, init=X[i])\n",
    "    print(torch.mean(i_mean))\n",
    "    i_mean = i_mean / torch.norm(i_mean, p=2)\n",
    "\n",
    "    # NOTE: if the intrinsic mean results in NaN, try changing the index i of the initialisation above\n",
    "    assert not torch.isnan(i_mean).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b157e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the PoS subspaces\n",
    "\n",
    "# map data from each PoS tag to the tangent space\n",
    "with torch.no_grad():\n",
    "    At = torch.cat([logarithmic_map(i_mean, x) for x in A], 0)\n",
    "    Nt = torch.cat([logarithmic_map(i_mean, x) for x in N], 0)\n",
    "    Vt = torch.cat([logarithmic_map(i_mean, x) for x in V], 0)\n",
    "    AVt = torch.cat([logarithmic_map(i_mean, x) for x in AV], 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mt = torch.mean(torch.cat([Nt, At, Vt, AVt], 0), 0)\n",
    "    la, WA = np.linalg.eigh(((1-0.5)/At.shape[0]*(At-mt).T@(At-mt) - 0.5 * (1/Nt.shape[0]*(Nt-mt).T@(Nt-mt) + 1/Vt.shape[0]*(Vt-mt).T@(Vt-mt) + 1/AVt.shape[0]*(AVt-mt).T@(AVt-mt)) ).detach().cpu().numpy())\n",
    "    ln, WN = np.linalg.eigh(((1-0.5)/Nt.shape[0]*(Nt-mt).T@(Nt-mt) - 0.5 * (1/At.shape[0]*(At-mt).T@(At-mt) + 1/Vt.shape[0]*(Vt-mt).T@(Vt-mt) + 1/AVt.shape[0]*(AVt-mt).T@(AVt-mt)) ).detach().cpu().numpy())\n",
    "    lv, WV = np.linalg.eigh(((1-0.5)/Vt.shape[0]*(Vt-mt).T@(Vt-mt) - 0.5 * (1/At.shape[0]*(At-mt).T@(At-mt) + 1/Nt.shape[0]*(Nt-mt).T@(Nt-mt) + 1/AVt.shape[0]*(AVt-mt).T@(AVt-mt)) ).detach().cpu().numpy())\n",
    "    lav, WAV = np.linalg.eigh(((1-0.5)/AVt.shape[0]*(AVt-mt).T@(AVt-mt) - 0.5 * (1/At.shape[0]*(At-mt).T@(At-mt) + 1/Nt.shape[0]*(Nt-mt).T@(Nt-mt) + 1/Vt.shape[0]*(Vt-mt).T@(Vt-mt)) ).detach().cpu().numpy())\n",
    "    \n",
    "    idxn = ln.argsort()[::-1]\n",
    "    WN = WN[:, idxn]\n",
    "    WN = torch.Tensor(WN).to('cuda')\n",
    "\n",
    "    idxa = la.argsort()[::-1]\n",
    "    WA = WA[:, idxa]\n",
    "    WA = torch.Tensor(WA).to('cuda')\n",
    "\n",
    "    idxv = lv.argsort()[::-1]   \n",
    "    WV = WV[:, idxv]\n",
    "    WV = torch.Tensor(WV).to('cuda')\n",
    "\n",
    "    idav = lav.argsort()[::-1]   \n",
    "    WAV = WAV[:, idav]\n",
    "    WAV = torch.Tensor(WAV).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef36d2",
   "metadata": {},
   "source": [
    "## PoS subspace visualisation\n",
    "\n",
    "I.e. plotting the first two coordinates of the data from the various PoS tags in each subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n = 10_000\n",
    "print(f'using n={n}')\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "def plot_subspace(ax, W, m, name, lam=0.5, marker='+', zorder=[1,2,3,4]):\n",
    "    Nc = (W.T @ (Nt[:n]-m).T).T.detach().cpu().numpy()\n",
    "    Ac = (W.T @ (At[:n]-m).T).T.detach().cpu().numpy()\n",
    "    Vc = (W.T @ (Vt[:n]-m).T).T.detach().cpu().numpy()\n",
    "    AVc = (W.T @ (AVt[:n]-m).T).T.detach().cpu().numpy()\n",
    "\n",
    "    ax.scatter(Nc[:, 0], Nc[:, 1], marker=marker, c='red', alpha=0.5, label='N', zorder=zorder[0], rasterized=True)\n",
    "    ax.scatter(Ac[:, 0], Ac[:, 1], marker=marker, c='blue', alpha=0.5, label='A', zorder=zorder[1], rasterized=True)\n",
    "    ax.scatter(Vc[:, 0], Vc[:, 1], marker=marker, c='green', alpha=0.5, label='V', zorder=zorder[2], rasterized=True)\n",
    "    ax.scatter(AVc[:, 0], AVc[:, 1], marker=marker, c='orange', alpha=0.5, label='AV', zorder=zorder[3], rasterized=True)\n",
    "    \n",
    "    abv = name[0] if name != 'Adverb' else 'R'\n",
    "    ax.set_xlabel(f'${{\\mathbf{{w}}_{abv}}}^T_1 \\mathbf{{z}}$')\n",
    "    ax.set_ylabel(f'${{\\mathbf{{w}}_{abv}}}^T_2 \\mathbf{{z}}$')\n",
    "    \n",
    "    ax.set_title(f'{name}-specific space, $\\lambda={lam}$')\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    \n",
    "plot_subspace(ax1, WN, mt, marker='o', name='Noun')\n",
    "plot_subspace(ax2, WA, mt, marker='o', name='Adjective', zorder=[3,1,2,4])\n",
    "plot_subspace(ax3, WV, mt, marker='o', name='Verb', zorder=[3,2,1,4])\n",
    "plot_subspace(ax4, WAV, mt, marker='o', name='Adverb', zorder=[2,3,4,1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d99a85",
   "metadata": {},
   "source": [
    "## Adj/Noun subspace projection\n",
    "\n",
    "Projecting onto the orthogonal complements of the noun & adjective subspaces. Example prompts in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338b937",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = 8\n",
    "    \n",
    "    ############### \n",
    "    # caption = \"A photo of Karl Marx in a disney movie\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of Virginia Woolf in the style of Impressionism\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of Socrates in a pixar movie\" ; ks = [(768, 768)]\n",
    "    # caption = \"Marcus Aurelius in a pixar movie\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of Ada Lovelace as a cartoon\" ; ks = [(768, 768)]\n",
    "    # caption = \"A drawing of Charles Darwin in the style of M.C. Escher\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of a multicoloured penguin\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of a snowy NYC\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of a dying tree\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of a sunny city\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of a rainy NYC\" ; ks = [(768, 768)]\n",
    "    # caption = \"A photo of snowy London\" ; ks = [(768, 768)]\n",
    "\n",
    "    ############### visually polysemous phrases\n",
    "    caption = \"Vincent van Gogh\" ; ks = [(768, 32)]\n",
    "    # caption = \"M.C. Escher\" ; ks = [(768, 32)]\n",
    "    # caption = \"Claude Monet\" ; ks = [(768, 32)]\n",
    "    # caption = 'David Hockney' ; ks = [(768, 32)]\n",
    "    # caption = \"Jackson Pollock\" ; ks = [(768, 32)]\n",
    "    # caption = 'J. M. W. Turner' ; ks = [(768, 32)]\n",
    "    # caption = \"Roy Lichtenstein\" ; ks = [(768, 32)]\n",
    "    # caption = \"Andy Warhol\" ; ks = [(768, 32)]\n",
    "    # caption = \"Edward Hopper\" ; ks = [(768, 32)]\n",
    "    # caption = \"Katsushika Hokusai\" ; ks = [(768, 32)]\n",
    "    # caption = \"Rothko\" ; ks = [(768, 32)]\n",
    "    # caption = \"Takashi Murakami\" ; ks = [(768, 32)]\n",
    "    \n",
    "    \n",
    "    seed = np.random.choice([0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    seed = 0\n",
    "    random.seed(seed) ; torch.manual_seed(seed) ; np.random.seed(seed); torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    #######################################################\n",
    "    # Paella scaffold: sampling\n",
    "    t5, clip_text, clip_image = False, True, False  # decide which conditionings to use for the sampling\n",
    "    use_prior = False  # whether to use generate clip image embeddings with the prior or to use image embeddings from given images defined in the cell above\n",
    "    latent_shape = (batch_size, 64, 64)  # latent shape of the generated image, we are using an f4 vqgan and thus sampling 64x64 will result in 256x256\n",
    "    prior_timesteps, prior_cfg, prior_sampler, clip_embedding_shape = 60, 3.0, \"ddpm\", (latent_shape[0], 1024)\n",
    "    text = tokenizer.tokenize([caption] * latent_shape[0]).to(device)\n",
    "    #######################################################\n",
    "    \n",
    "    sampled_list = []\n",
    "    \n",
    "    clip_text_tokens_uncond = tokenizer.tokenize([\"\"] * len(text)).to(device)\n",
    "    t5_embeddings_uncond = embed_t5([\"\"] * len(text), t5_tokenizer, t5_model, device=device)\n",
    "    t5_embeddings = t5_embeddings_uncond\n",
    "\n",
    "    clip_text_embeddings = clip_model.encode_text(text)\n",
    "    clip_text_embeddings_uncond = clip_model.encode_text(clip_text_tokens_uncond)\n",
    "    clip_image_embeddings = None\n",
    "    \n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for mod in ['Original', 'Noun subspace orth. projection', 'Adj. subspace orth. projection']:\n",
    "            values = [(1024, 1024)] if mod == 'Original' else ks\n",
    "            for k in values:\n",
    "                ce = clip_text_embeddings.clone()\n",
    "                ###############################################\n",
    "                cn = torch.norm(ce, p=2, dim=1).unsqueeze(1)\n",
    "                ce = ce / cn\n",
    "                ce = logarithmic_map(i_mean, ce)\n",
    "            \n",
    "                PA_o = torch.eye(1024).to('cuda') - WA[:, :k[0]] @ WA[:, :k[0]].T\n",
    "                PN_o = torch.eye(1024).to('cuda') - WN[:, :k[1]] @ WN[:, :k[1]].T\n",
    "\n",
    "                # remove component from original\n",
    "                if mod == 'Adj. subspace orth. projection': ce = (PA_o @ (ce-mt).T).T + mt\n",
    "                if mod == 'Noun subspace orth. projection': ce = (PN_o @ (ce-mt).T).T + mt\n",
    "\n",
    "                ce = exponential_map(i_mean, ce)\n",
    "                ce *= cn  # rescale by original length\n",
    "                ##########################################################\n",
    "\n",
    "                # Paella defaults; attention reweight\n",
    "                attn_weights = torch.ones((t5_embeddings.shape[1])); attn_weights[-4:] = 0.4; attn_weights[:-4] = 1.2; attn_weights = attn_weights.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Paella defaults\n",
    "                    sampled_tokens, _ = sample(model, seed, init_noise, model_inputs={'byt5': t5_embeddings.clone(), 'clip': ce.clone(), 'clip_image': clip_image_embeddings}, unconditional_inputs={'byt5': t5_embeddings_uncond.clone(), 'clip': clip_text_embeddings_uncond.clone(), 'clip_image': None},\n",
    "                                                    temperature=(1.2, 0.2), cfg=(8,8), steps=32, renoise_steps=26, latent_shape=latent_shape, t_start=1.0, t_end=0.0,\n",
    "                                                                  mode=\"multinomial\", sampling_conditional_steps=None, attn_weights=attn_weights)\n",
    "\n",
    "                sampled = decode(sampled_tokens)\n",
    "                sampled_list += [sampled]\n",
    "                title = f'\"{caption}\", {mod}'\n",
    "                showimages(sampled.float(), title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c2a98",
   "metadata": {},
   "source": [
    "## Style-blocking adjective projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = 8\n",
    "    \n",
    "    ############### \n",
    "    k = 800 # <- 800 is a good default, but could benefit from tuning for some prompts\n",
    "    caption = \"A painting of a mountain in the style of Van Gogh\"\n",
    "    # caption = \"A Gauguin painting of Einstein\"\n",
    "    # caption = \"A painting of the Eiffel Tower in the style of Rothko\"\n",
    "    # caption = \"A portrait of a woman in the style of Roy Lichtenstein\"\n",
    "    # caption = \"A David Hockney painting of a house\"\n",
    "    # caption = \"A photo of the sky in the style of Van Gogh\"\n",
    "    # caption = \"A Shiba Inu in the style of Van Gogh\"\n",
    "    # caption = \"A portrait painting of a man in the style of Picasso\"\n",
    "\n",
    "    seed = np.random.choice([0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    seed = 0\n",
    "    random.seed(seed) ; torch.manual_seed(seed) ; np.random.seed(seed); torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    #######################################################\n",
    "    # Paella scaffold: sampling\n",
    "    t5, clip_text, clip_image = False, True, False  # decide which conditionings to use for the sampling\n",
    "    use_prior = False  # whether to use generate clip image embeddings with the prior or to use image embeddings from given images defined in the cell above\n",
    "    latent_shape = (batch_size, 64, 64)  # latent shape of the generated image, we are using an f4 vqgan and thus sampling 64x64 will result in 256x256\n",
    "    prior_timesteps, prior_cfg, prior_sampler, clip_embedding_shape = 60, 3.0, \"ddpm\", (latent_shape[0], 1024)\n",
    "    text = tokenizer.tokenize([caption] * latent_shape[0]).to(device)\n",
    "    #######################################################\n",
    "    \n",
    "    sampled_list = []\n",
    "    \n",
    "    clip_text_tokens_uncond = tokenizer.tokenize([\"\"] * len(text)).to(device)\n",
    "    t5_embeddings_uncond = embed_t5([\"\"] * len(text), t5_tokenizer, t5_model, device=device)\n",
    "    t5_embeddings = t5_embeddings_uncond\n",
    "\n",
    "    clip_text_embeddings = clip_model.encode_text(text)\n",
    "    clip_text_embeddings_uncond = clip_model.encode_text(clip_text_tokens_uncond)\n",
    "    clip_image_embeddings = None\n",
    "    \n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for mod in ['Original', 'Style block']:\n",
    "            values = 1024 if mod == 'Original' else k\n",
    "            ce = clip_text_embeddings.clone()\n",
    "            ###############################################\n",
    "            cn = torch.norm(ce, p=2, dim=1).unsqueeze(1)\n",
    "            ce = ce / cn\n",
    "            ce = logarithmic_map(i_mean, ce)\n",
    "        \n",
    "            PA_o = torch.eye(1024).to('cuda') - WA[:, :k] @ WA[:, :k].T\n",
    "\n",
    "            # remove adjective subspace component\n",
    "            if mod == 'Style block': ce = (PA_o @ (ce-mt).T).T + mt\n",
    "\n",
    "            ce = exponential_map(i_mean, ce)\n",
    "            ce *= cn  # rescale by original length\n",
    "            ##########################################################\n",
    "\n",
    "            # Paella defaults; attention reweight\n",
    "            attn_weights = torch.ones((t5_embeddings.shape[1])); attn_weights[-4:] = 0.4; attn_weights[:-4] = 1.2; attn_weights = attn_weights.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Paella defaults\n",
    "                sampled_tokens, _ = sample(model, seed, init_noise, model_inputs={'byt5': t5_embeddings.clone(), 'clip': ce.clone(), 'clip_image': clip_image_embeddings}, unconditional_inputs={'byt5': t5_embeddings_uncond.clone(), 'clip': clip_text_embeddings_uncond.clone(), 'clip_image': None},\n",
    "                                                temperature=(1.2, 0.2), cfg=(8,8), steps=32, renoise_steps=26, latent_shape=latent_shape, t_start=1.0, t_end=0.0,\n",
    "                                                                mode=\"multinomial\", sampling_conditional_steps=None, attn_weights=attn_weights)\n",
    "\n",
    "            sampled = decode(sampled_tokens)\n",
    "            sampled_list += [sampled]\n",
    "            title = f'\"{caption}\", {mod}'\n",
    "            showimages(sampled.float(), title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f0084",
   "metadata": {},
   "source": [
    "# Custom subspace projection\n",
    "\n",
    "$\\color{red}{\\texttt{Content Warning}}$: default prompts for the \"gore\" subspace in the paper produce gory, bloody original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dict import visual_themes\n",
    "\n",
    "for theme in visual_themes.keys():\n",
    "    print(f'Encoding theme: {theme}...')\n",
    "    An = []\n",
    "    for word in visual_themes[theme]['custom_dict']:\n",
    "        tokenized_text = tokenizer.tokenize([word]).to(device)\n",
    "        with torch.inference_mode():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                text_embeddings = clip_model.encode_text(tokenized_text)\n",
    "            if norm: text_embeddings /= torch.norm(text_embeddings, 2)\n",
    "            An += [text_embeddings[0]]\n",
    "    An = torch.stack(An, 0).float()\n",
    "\n",
    "    # map to tangent space\n",
    "    Ant = torch.cat([logarithmic_map(i_mean, x) for x in An], 0)\n",
    "    \n",
    "    l, WAn = np.linalg.eigh(((1-0.5)/Ant.shape[0]*(Ant-mt).T@(Ant-mt) - 0.5 * (1/Nt.shape[0]*(Nt-mt).T@(Nt-mt) + 1/At.shape[0]*(At-mt).T@(At-mt) + 1/Vt.shape[0]*(Vt-mt).T@(Vt-mt) + 1/AVt.shape[0]*(AVt-mt).T@(AVt-mt)) ).detach().cpu().numpy())\n",
    "\n",
    "    idxn = l.argsort()[::-1]   \n",
    "    WAn = WAn[:, idxn]\n",
    "    visual_themes[theme]['subspace'] = torch.Tensor(WAn).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81d7edef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't5_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28200\\2277883753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mclip_text_tokens_uncond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mt5_embeddings_uncond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membed_t5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt5_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt5_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mt5_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt5_embeddings_uncond\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't5_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = 2\n",
    "    \n",
    "    ############### \n",
    "    caption = \"A photo of a bloody rabbit carcass\" ; theme = 'gore' ; k = 128\n",
    "    # caption = \"A painting of a beach in the style of Qi Baishi\" ; theme = 'artist' ; k = 512\n",
    "\n",
    "    seed = np.random.choice([0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    random.seed(seed) ; torch.manual_seed(seed) ; np.random.seed(seed); torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    #######################################################\n",
    "    # Paella scaffold: sampling\n",
    "    t5, clip_text, clip_image = False, True, False  # decide which conditionings to use for the sampling\n",
    "    use_prior = False  # whether to use generate clip image embeddings with the prior or to use image embeddings from given images defined in the cell above\n",
    "    latent_shape = (batch_size, 64, 64)  # latent shape of the generated image, we are using an f4 vqgan and thus sampling 64x64 will result in 256x256\n",
    "    prior_timesteps, prior_cfg, prior_sampler, clip_embedding_shape = 60, 3.0, \"ddpm\", (latent_shape[0], 1024)\n",
    "    text = tokenizer.tokenize([caption] * latent_shape[0]).to(device)\n",
    "    #######################################################\n",
    "    \n",
    "    sampled_list = []\n",
    "    \n",
    "    clip_text_tokens_uncond = tokenizer.tokenize([\"\"] * len(text)).to(device)\n",
    "    t5_embeddings_uncond = embed_t5([\"\"] * len(text), t5_tokenizer, t5_model, device=device)\n",
    "    t5_embeddings = t5_embeddings_uncond\n",
    "\n",
    "    clip_text_embeddings = clip_model.encode_text(text)\n",
    "    clip_text_embeddings_uncond = clip_model.encode_text(clip_text_tokens_uncond)\n",
    "    clip_image_embeddings = None\n",
    "    \n",
    "    init_noise = torch.randint(0, model.num_labels, size=latent_shape, device=device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for mod in ['Original', 'Theme block']:\n",
    "            values = 1024 if mod == 'Original' else k\n",
    "            ce = clip_text_embeddings.clone()\n",
    "            ###############################################\n",
    "            cn = torch.norm(ce, p=2, dim=1).unsqueeze(1)\n",
    "            ce = ce / cn\n",
    "            ce = logarithmic_map(i_mean, ce)\n",
    "        \n",
    "            W = visual_themes[theme]['subspace']\n",
    "            P_o = torch.eye(1024).to('cuda') - W[:, :k] @ W[:, :k].T\n",
    "\n",
    "            # remove subspace component\n",
    "            if mod == 'Theme block': ce = (P_o @ (ce-mt).T).T + mt\n",
    "\n",
    "            ce = exponential_map(i_mean, ce)\n",
    "            ce *= cn  # rescale by original length\n",
    "            ##########################################################\n",
    "\n",
    "            # Paella defaults; attention reweight\n",
    "            attn_weights = torch.ones((t5_embeddings.shape[1])); attn_weights[-4:] = 0.4; attn_weights[:-4] = 1.2; attn_weights = attn_weights.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Paella defaults\n",
    "                sampled_tokens, _ = sample(model, seed, init_noise, model_inputs={'byt5': t5_embeddings.clone(), 'clip': ce.clone(), 'clip_image': clip_image_embeddings}, unconditional_inputs={'byt5': t5_embeddings_uncond.clone(), 'clip': clip_text_embeddings_uncond.clone(), 'clip_image': None},\n",
    "                                                temperature=(1.2, 0.2), cfg=(8,8), steps=32, renoise_steps=26, latent_shape=latent_shape, t_start=1.0, t_end=0.0,\n",
    "                                                                mode=\"multinomial\", sampling_conditional_steps=None, attn_weights=attn_weights)\n",
    "\n",
    "            sampled = decode(sampled_tokens)\n",
    "            title = f'\"{caption}\", {mod}'\n",
    "\n",
    "            #################### gaussian blur sensitive original images\n",
    "            if theme == 'gore' and mod == 'Original':\n",
    "                print(f'INFO: Gaussian blur-ing original {theme} images')\n",
    "                sampled = torchvision.transforms.functional.gaussian_blur(sampled, kernel_size=19)\n",
    "                title = title + ' [Gaussian blurred]'\n",
    "\n",
    "            sampled_list += [sampled]\n",
    "            showimages(sampled.float(), title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10deefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DenoiseGIT_sampling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
